{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68372e47-6c96-4180-9253-714b6dddb6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmdb\n",
    "import rasterio\n",
    "import safetensors\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from safetensors.numpy import deserialize, load_file, load\n",
    "\n",
    "def read_single_band_raster(path):\n",
    "    with rasterio.open(path) as r:\n",
    "        return r.read(1)\n",
    "\n",
    "p = Path(\"tiffs/BigEarthNet\")\n",
    "source_data = {file: read_single_band_raster(file) for file in p.glob(\"**/*.tif*\")}\n",
    "\n",
    "# code to create the directory\n",
    "# ./result/bin/encoder --bigearthnet-s1-root tiffs/BigEarthNet/S1/ --bigearthnet-s2-root tiffs/BigEarthNet/S2/ artifacts/\n",
    "env = lmdb.open(\"../artifacts/ben\", readonly=True)\n",
    "\n",
    "with env.begin(write=False) as txn:\n",
    "    cur = txn.cursor()\n",
    "    decoded_lmdb_data = {k.decode(\"utf-8\"): load(v) for (k, v) in cur}\n",
    "\n",
    "# The encoded data is nested inside of another safetensor dictionary, where the inner keys are derived from the band suffix\n",
    "decoded_values = [v for outer_v in decoded_lmdb_data.values() for v in outer_v.values()]\n",
    "\n",
    "# Simply check if the data remains identical, as this is the only _true_ thing I care about from the Python viewpoint\n",
    "# If the keys/order or anything else is wrong, it isn't part of the integration test but should be handled separately as a unit test!\n",
    "for (source_key, source_value) in source_data.items():\n",
    "    assert any(np.array_equal(source_value, decoded_value) for decoded_value in decoded_values), f\"Couldn't find data in the LMDB database that matches the data from: {source_key}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf1a7a1-6240-47ef-a15c-e45ab88a1362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_raster_bands(path):\n",
    "    \"\"\"\n",
    "    Given a path to a GeoTIFF return all bands as a dictionary,\n",
    "    where the key is the unformatted band index (starting from 1)\n",
    "    as a string and the value the array data\n",
    "    \"\"\"\n",
    "    with rasterio.open(path) as r:\n",
    "        return {str(i): r.read(i) for i in range(1, r.count + 1)}\n",
    "\n",
    "p = Path(\"tiffs/HySpecNet-11k\")\n",
    "source_file_data = {file: read_all_raster_bands(file) for file in p.glob(\"**/*SPECTRAL_IMAGE.TIF\")}\n",
    "assert len(source_file_data) > 0\n",
    "\n",
    "# code to create the directory\n",
    "# ./result/bin/encoder --hyspecnet-11k <PATH> hyspec_artifacts/\n",
    "env = lmdb.open(\"../artifacts/hyspecnet\", readonly=True)\n",
    "\n",
    "with env.begin(write=False) as txn:\n",
    "    cur = txn.cursor()\n",
    "    decoded_lmdb_data = {k.decode(\"utf-8\"): load(v) for (k, v) in cur}\n",
    "\n",
    "# The encoded data is nested inside of another safetensor dictionary, where the inner keys are derived from the band number as a string\n",
    "decoded_dicts = [d for d in decoded_lmdb_data.values()]\n",
    "\n",
    "# Simply check if the data remains identical, as this is the only _true_ thing I care about from the Python viewpoint\n",
    "# Here I iterate over all file name and raster data as dictionaries pairs\n",
    "# and then for each raster data dictionary iterate over all key-value pairs, where the key is the band name\n",
    "# in the same style as the LMDB file and check if the LMDB file contained a matching array from\n",
    "# a safetensors dictionary accessed via the shared band name as key.\n",
    "for (source_file, source_data_dict) in source_file_data.items():\n",
    "    for (source_key, source_data) in source_data_dict.items():\n",
    "        assert any(np.array_equal(source_data, decoded_dict[source_key]) for decoded_dict in decoded_dicts), f\"Couldn't find data in the LMDB database that matches the data from: {source_file}:{source_key}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ae1700-e61b-439f-a884-9003b47ecc16",
   "metadata": {},
   "source": [
    "## Optimizing access patterns\n",
    "\n",
    "Strictly speaking, we are not taking advantage of the lazy-loading API for our bigearthnet patches, as we are using `load` which internally calls `deserialize` on the byte string\n",
    "and iterates over all elements and adds them to the dictionary. But some quick testing has revealed that there is no major performance penalty, especially since we are loading most of the data.\n",
    "Only for HySpecNet, we can take advantage of this internal design style and directly add an `np.stack` to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db336102-60e1-4a4e-8d67-8adc532d8415",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "byte indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msafetensor_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mB01\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: byte indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "safetensor_dict[\"B01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d819f7d-4c0c-41f2-b874-b2a6c8d3d0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "key = 'ENMAP01-____L2A-DT0000004950_20221103T162438Z_001_V010110_20221118T145147Z-Y01460273_X03110438\n",
    "\n",
    "for i in range(1, 225):\n",
    "    decoded_lmdb_data[key][str(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bce7df-6aae-4c2d-92ca-f2f921a93268",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "for i in range(1, 22):\n",
    "    decoded_lmdb_data[key][str(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d974c0-531e-419e-9f69-656db858292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "np.stack([decoded_lmdb_data[key][str(i)] for i in range(1, 225)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3383c0a5-2130-4412-9ff7-94666b7a5f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "np.stack([decoded_lmdb_data[key][str(i)] for i in range(1, 22)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a427582-9cc8-4493-b909-5791160da3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "a = np.zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7759f06d-75ef-40ad-a6b9-afed13d5a352",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPPORTED_BANDS = list(i for i in range(5))\n",
    "np.stack([decoded_lmdb_data[key][str(i)] for i in range(1, 22) if i in SUPPORTED_BANDS], axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec80ead-1c04-42d7-99de-92df4c3a7a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "a = np.zeros((224, 128, 128))\n",
    "for i in range(1, 224):\n",
    "    a[i-1] = decoded_lmdb_data[key][str(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3341546c-0fe8-4f52-8b66-6190db7002e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "minimum_value = 0\n",
    "maximum_value = 10000\n",
    "\n",
    "clipped = np.stack([decoded_lmdb_data[key][str(i)] for i in range(1, 225)], axis=0).clip(min=minimum_value, max=maximum_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a5702d-377b-4218-b40b-99339ecc6946",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "out_data = (clipped - minimum_value) / (maximum_value - minimum_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b256f6b1-10c0-4219-a00e-6a45ec74499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "out_dataf = out_data.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7be06a-3d12-44af-aa82-c233d65824ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# astype without explicit intermediate value is just as fast as with intermediate value\n",
    "out_data = ((clipped - minimum_value) / (maximum_value - minimum_value)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c8e0f7-03d5-4ed4-bd44-99d5415447ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a single patch it takes around 10ms per patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f82b44-6cad-44b4-963a-9d39897c2f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.72 batches / sek bei Martin for entire training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4368b5e7-52f8-4c31-a634-74f0e3717551",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example\n",
    "\n",
    "import lmdb\n",
    "import safetensors\n",
    "from safetensors.numpy import _getdtype\n",
    "from pathlib import Path\n",
    "\n",
    "# path to the encoded dataset/output of rico-hdl\n",
    "encoded_path = Path(\"../artifacts/ben/\")\n",
    "\n",
    "# Make sure to only open the environment once\n",
    "env = lmdb.open(str(encoded_path), readonly=True)\n",
    "\n",
    "with env.begin() as txn:\n",
    "  tensor_dict = txn.get('S2A_MSIL2A_20180526T100031_N9999_R122_T34WFU_14_23'.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "090edf91-de06-4c35-babd-564843d8e727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0msafetensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Opens a safetensors lazily and returns tensors as asked\n",
       "\n",
       "Args:\n",
       "    data (`bytes`):\n",
       "        The byte content of a file\n",
       "\n",
       "Returns:\n",
       "    (`List[str, Dict[str, Dict[str, any]]]`):\n",
       "        The deserialized content is like:\n",
       "            [(\"tensor_name\", {\"shape\": [2, 3], \"dtype\": \"F32\", \"data\": b\"\\0\\0..\" }), (...)]\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load(tensor_dict)\n",
    "safetensors.deserialize??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3d26cc74-2d0e-4209-80b1-a9225eda8c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.9 µs ± 903 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# 31 us only for RGB\n",
    "# 39 us for everything\n",
    "# 28 us for B01 -> almost no difference whatsoever\n",
    "result = {}\n",
    "for k, v in safetensors.deserialize(tensor_dict):\n",
    "    if k not in [\"B01\"]:\n",
    "        continue\n",
    "    dtype = _getdtype(v[\"dtype\"])\n",
    "    arr = np.frombuffer(v[\"data\"], dtype=dtype).reshape(v[\"shape\"])\n",
    "    result[k] = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1c9dbb25-40fb-42b0-a62b-1e0e77efa283",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_path = \"../artifacts/ben\"\n",
    "env = lmdb.open(str(encoded_path), readonly=True)\n",
    "\n",
    "with env.begin() as txn:\n",
    "  # string encoding is required to map the string to an LMDB key\n",
    "  safetensor_dict = load(txn.get(\"S2A_MSIL2A_20180526T100031_N9999_R122_T34WFU_14_23\".encode()))\n",
    "\n",
    "rgb_bands = [\"B04\", \"B03\", \"B02\"]\n",
    "rgb_tensor = np.stack([safetensor_dict[b] for b in rgb_bands])\n",
    "assert rgb_tensor.shape == (3, 120, 120) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e28fb2e4-9f29-4de0-bacf-0a1142c10487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 120, 120)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dabde15b-aa54-45e9-bc36-3518bca1aafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmdb\n",
    "import numpy as np\n",
    "# import desired deep learning library:\n",
    "# numpy, torch, tensorflow, paddle, flax, mlx\n",
    "from safetensors.numpy import load\n",
    "from pathlib import Path\n",
    "\n",
    "# Make sure to only open the environment once\n",
    "# and not everytime an item is accessed.\n",
    "encoded_path = \"../artifacts/hyspecnet\"\n",
    "env = lmdb.open(str(encoded_path), readonly=True)\n",
    "\n",
    "with env.begin() as txn:\n",
    "  # string encoding is required to map the string to an LMDB key\n",
    "  safetensor_dict = load(txn.get(\"ENMAP01-____L2A-DT0000004950_20221103T162438Z_001_V010110_20221118T145147Z-Y01460273_X04390566\".encode()))\n",
    "\n",
    "hyspecnet_bands = range(1, 225)\n",
    "# recommendation from HySpecNet-11k paper \n",
    "skip_bands = [126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 160, 161, 162, 163, 164, 165, 166]\n",
    "tensor = np.stack([safetensor_dict[f\"B{k}\"] for k in hyspecnet_bands if k not in skip_bands])\n",
    "assert tensor.shape == (202, 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b6586fe-fc73-48ff-8afa-7840e06d5675",
   "metadata": {},
   "outputs": [],
   "source": [
    "with env.begin() as txn:\n",
    "    cur = txn.cursor()\n",
    "    decoded_lmdb_data = {k.decode(\"utf-8\"): load(v) for (k, v) in cur}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602669fe-7afa-491b-b9ce-6c350bacb8df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61ced85-0361-4545-98a0-cb5786597ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
